
<!-- MarkdownTOC -->

- 1.Decision Tree
    - 1.1 决策树模型
    - 1.2 决策树学习基本算法
    - 1.3 决策树节点分裂算法
        - 1.3.1 信息熵
        - 1.3.2 信息增益
        - 1.3.3 信息增益率
        - 1.3.4 CART
            - 1.3.4.1 CART\(Classification And Regression Tree\)模型介绍
            - 1.3.4.2 CART节点分裂准则-Gini指数
    - 1.4 决策树剪枝
    - 1.5 决策树模型优缺点
    - 1.6 决策树在`sklearn`中的实现简单用法
        - 1.6.1 分类
        - 1.6.2 回归
- 2.AdaBoost
    - 2.1 集成学习\(ensemble learning\)
    - 2.2 AdaBoost模型
- 3.GBM
    - 3.1 GBM模型原理
        - 3.1.1 函数估计问题
        - 3.1.2 梯度提升模型\(GBM\)
        - 3.1.3 梯度提升模型算法的应用
    - 3.2 GBM模型调参\(Python\)
        - 3.2.1 参数类型
        - 3.2.2 调参策略
- 4.XGBoost
    - 4.1 XGBoost模型
    - 4.2 XGBoost模型理论推导
        - **模型目标函数**
        - **模型目标**
        - **模型损失函数定义**
        - **模型正则项定义**
        - **模型目标函数求解**
        - 模型中决策树节点分裂算法
            - 贪心算法\(Exact Greedy Algorithm for split finding\)
            - 近似算法\(Approximate Algorithm for split finding\)
        - 自定义损失函数
        - XGBoost调参
    - 3.3 XGBoost 应用
        - 3.3.1 XGBoost Python API
        - 3.3.2 XGBoost R API
- 5.LightGBM模型理论
    - 5.1 LightGBM模型算法详解
    - 5.2 LightGBM使用方法
- 6.Random Forest
    - 6.1 随机森林的基学习器\(base learner\)
    - 6.2 随机森林的随机性
    - 6.3 随机森林的构建过程
        - 6.3.1 为每棵决策树Bootstrap抽样产生训练集
        - 6.3.2 构建每棵决策树
        - 6.3.3 随机森林的形成
    - 6.4 随机森林的优缺点
    - 6.5 随机森林的特征选择功能
        - 6.5.1 特征选择
            - 6.5.1.1 特征选择的步骤
            - 6.5.1.2 特征重要性的估计方法
        - 6.5.2 利用随机森林进行特征选择
    - 6.6 随机森林的性质、性能、参数
        - 6.6.1 Random Forest 性质讨论
        - **3.2 Random Forest 性能评价指标**
        - 6.6.2 随机森林参数设置

<!-- /MarkdownTOC -->

* Decision Tree
	- [信息熵]()
	- [决策树]()
* Gradient boosting [Wiki](https://en.wikipedia.org/wiki/Gradient_boosting)
    - AdaBoost
        + [论文]()
        + [slides]()
        + [API文档]()
        + [Github]()
    - GBDT/GBM
        + [论文]()
        + [slides]()
        + [API文档]()
        + [Github]()
    - XGBoost (eXtreme Gradient Boosting)
        + [论文](https://arxiv.org/pdf/1603.02754.pdf)
        + [slides](https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf)
        + [API文档](https://xgboost.readthedocs.io/en/latest/index.html)
        + [Github]()
    - LightGBM
        + [论文]()
        + [slides]()
        + [API文档]()
        + [Github]()
    - CatBoost
        + [论文]()
        + [slides]()
        + [API文档]()
        + [Github]()
* Random Forest
    - [论文]()
    - [slides]()
    - [API文档]()
    - [Github]()



# 1.Decision Tree

## 1.1 决策树模型

> **决策树是一种典型的基本分类器，从本质上讲，决策树的分类思想是产生一系列规则，然后通过这些规则进行数据分析的数据挖掘过程。该分类器的生成和决策过程分为三个部分：**

1. 首先，通过对训练集进行递归分析，生成一棵形状如倒立的树状结构；
2. 然后，分析这棵树从根节点到叶节点的路径，产生一系列规则；
3. 最后，根据这些规则对新数据进行分类预测。

> **决策树为一个树状模型，树中包含三种节点：根节点、(中间)内部节点、叶节点：**

* 根节点包含样本全集；
* 树的每个节点表示样本对象的属性；
* 从每个节点出发的分叉路径代表某个可能的属性值；
* 从根节点出发，经过若干中间节点后，到达叶节点的路径表示某个规则，整个树表示由训练样本决定的规则集合；
* 每个叶节点对应从根节点到该叶节点所经历的路径所表示的对象的值。即叶节点对应于决策结果，其他每个节点则对应于一个属性测试(分割变量选择、分割节点选择)；
* 每个节点包含的样本集合根据属性测试的结果被划分到子节点中；

> **决策树模型构建的问题基本可以归结为两点：**


* 生成树(split)
    - 选择变量作为分割变量(分割节点)；
        - 决策树的生成过程就是 使用满足划分准则的特征不断的将数据集划分为纯度更高，不确定性更小的子集的过程。
对于当前数据集的每一次的划分，都希望根据某特征划分之后的各个子集的纯度更高，不确定性更小；
        - 特征选择准则：
            - 使用某特征对数据集划分之后，各数据子集的纯度要比划分前的数据集D的纯度高(不确定性要比划分前数据集D的不确定性低)；
            - 划分后的纯度为各数据子集的纯度的加和(子集占比\*子集的经验熵)；
            - 度量划分前后的纯度变化用子集的纯度之和与划分前的数据集D的纯度进行对比；
    - 如何度量划分数据集前后的数据集的纯度以及不确定性呢： 
        - 回归树
            - 树节点分裂准则
                - 预测误差，计算分裂前后集合的误差得分，取误差得分最小的进行分裂
                    - 均方误差
                    - 对数误差 
            - 树叶节点预测值
                - 节点内样本目标变量均值
                - 通过最优化算法计算得到(XGBoost)
        - 分类树
            - 树节点分裂准则
                - 信息增益：ID3模型
                - 信息增益率：C4.5模型
                - GINI指数：CART模型
            - 树叶节点预测值
                - 通过投票法产生的类别值(少数服从多数)
* 树的剪枝(prune)
    - 预剪枝
    - 后剪枝
    - 正则化方法(XGBoost)

## 1.2 决策树学习基本算法

> 
* 输入：
    - 训练集：$D=\{(x_{1}, y_{1}),(x_{2}, y_{2}),\ldots,(x_{n}, y_{n})\};$
    - 变量集：$A=\{a_{1}, a_{2},\ldots,a_{p}\}$ 
* 过程：
    - 函数：$TreeGenerate(D, A)$
    1. 生成根节点node；
    2. **if**【$D$ 中样本全属于同一类别 $C$】： **then**
        - 将根节点node标记为$C$类叶节点，**return**
    4. **end if**；
    5. **if**【 $A=\emptyset$ **or** $D$ 中样本在 $A$ 上取值相同】： **then**
        - 将根节点node标记为叶节点，其类别标记为 $D$ 中样本数最多的类，**return**
    7. **end if**；
    8. **else**
        - 从 $A$ 中选择最优划分变量 $a_{*}$；
        - **for**【$a_{*}$ 的每一个值 $a_{*}^{v}$】 **do**：
            - 为node生成一个分支；令 $D_{v}$ 表示 $D$ 中在 $a_{*}$ 上取值为 $a_{*}^{v}$ 的样本子集；
                - **if**【$D_{v}=\emptyset$】 **then**
                    - 将根节点node标记为叶节点，其类别标记为 $D$ 中样本数最多的类；
                - **else**
                    - 以 $TreeGenerate(D, A \& \{a_{*}\})$ 为分支节点
                - **end if**；
        - **end for**；
* 输出：
    - 以node为根节点的一棵决策树
> 

## 1.3 决策树节点分裂算法

### 1.3.1 信息熵

> * **信息熵(information entropy)**
    - 度量样本集合(随机变量)纯度(不确定性)最常用的指标；
    - 信息熵的值越小，则样本集合的纯度越高，信息熵的值越大，则样本集合的不确定性就越大；

假设样本集合 $D$ 中第 $k$ 类样本所占的比例为 $p_{k}, (k=1, 2, \ldots, |\mathcal{Y}|)$，则样本集合 $D$ 的信息熵为：

$$Entropy(D) = -\sum^{|\mathcal{Y}|}_{k=1}p_{k}log_{2}p_{k}$$

特别地，对于二分类的情况，$D$ (根节点) 中只有两个类别，假设每个类别在样本集中的比例为: 正例的比例 $p_{+}$，以及负例的比例 $p_{-}$，则样本集合 $D$ (根节点)的信息熵为：

$$Entropy(D) = -p_{+}\log_2 p_{+} - p_{-}\log_{2} p_{-}$$

### 1.3.2 信息增益

> * **信息增益(information gain)**
    - 信息增益主要作为 **ID3决策树**[Quinlan,1986] 的变量选择分裂准则，这里对节点的划分不是二叉划分，是多叉划分；
    - 信息增益越大，则使用相应变量来进行划分所获得的“纯度提升”越大；

假设离散变量 $a$ 有 $V$ 个可能的取值 $\{a^{1}, a^{2}, \ldots, a^{V}\}$，若使用变量 $a$ 对样本集合 $D$ 进行划分，则会产生 $V$ 个分支节点，其中第 $v$ 个分支节点包含了 $D$ 中所有在变量 $a$ 上取值为 $a^{v}$ 的样本，记为 $D^{v}$， 易知分裂后的各个样本集合 $D^{1},D^{2},\ldots D^{v}, D^{V}$ 的信息熵为：

 $$\sum^{V}_{v=1}Entropy(D^{v})$$

再考虑到不同的分支节点所包含的样本数不同，所以给分支节点赋权重 $\frac{|D^{v}|}{|D|}$，即样本数越多的分支节点影响越大，于是可计算出用变量 $a$ 进行划分所获得的“信息增益”为：

 $$Gain(D, a)=Entropy(D)-\sum^{V}_{v=1}\frac{|D^{v}|}{|D|}Entropy(D^{v})$$

最终选择获得信息增益最大的变量为：

 $$a_{*}=\underset{a\in A}{argmax} Gain(D, a)$$

### 1.3.3 信息增益率

> * 信息增益准则对可取值数目多的变量有所偏好，为减少这种偏好可能带来的不利影响，**C4.5决策树算法**[Quinlan,1993]不直接使用信息增益，使用**信息增益率(gain ratio)**来选择最优的划分变量；
> * 信息增益率准则对变量取值数目较少的属性有所偏好，因此C4.5并不是直接选择信息增益率最大的变量，而是使用了一个启发式的：先从变量集合中找出信息增益率高于平均水平的变量，再从中选择信息增益率最高的；
> * C4.5仍然是多分叉树，下面给出信息增益率的表达形式，它是在信息增益的基础上进行改进得到的；

$$Gain\_ratio(D, a)=\frac{Gain(D, a)}{IV(a)}$$

其中：

$IV(a)$ 称为变量 $a$ 的“固有值(intrinsic value)”，变量 $a$ 可能取值的数目越多(即 $V$ 越大)，则 $IV(a)$ 的值通常会越大：

$$IV(a)=-\sum^{V}_{v=1}\frac{|D^{v}|}{|D|}log_{2}\frac{|D^{v}|}{|D|}$$

### 1.3.4 CART

#### 1.3.4.1 CART(Classification And Regression Tree)模型介绍

* **CART(Classification And Regression Tree 分类与回归树)：** 
    - CART是树模型算法的一种，它先从自变量集合中寻找最佳分割变量和最佳分割点(基尼指数)，将数据划分为**两组**。针对分组后的数据将上述步骤重复下去，直到满足某种停止条件。这样反复分隔数据后使分组后的数据变得一致，纯度较高。同时可自动探测出复杂数据的潜在结构、重要模式和关系，探测出的知识又可用来构造精确和可靠地预测模型。关于分类与回归树的深入理论,可以参考Breiman等人的著作(CART)。
    - CART模型的构建可以看作是一个对变量进行递归分割选择的过程。递归分割，顾名思义也就是对变量进行逐层分隔，直到分割结果满足某种条件才停下来，这里“分割的结果”可能是得到一些分类值(分类树)，也可能是一些描述统计量或预测值(回归树)。建立的CART模型可分为**分类树(classification tree)**和**回归树(regression tree)**两种：
        - 分类树用于因变量为分类数据的情况，树的末端为因变量的分类值；
        - 回归树则可以用于因变量为连续变量的情况，树的末端可以给出相应类别中的因变量描述或预测；
* **CART模型构建具体步骤：**
    - **1.对所有自变量和所有分割点进行评估，选择最优分割变量，生成一棵规模较大的CART树** 
        - 最佳的选择是使得分割后组内的数据纯度更高，即组内数据的目标变量变异更小。这种纯度可通过Gini指数或熵Entropy来度量；
    - **2.对树进行剪枝(prune)** 
        - 要兼顾树的规模和误差的大小，因此通常会使用CP参数(complexity parameter)来对树的复杂度进行控制，使预测误差和树的规模都尽可能小。通常的做法是，先建立一个划分较细较为复杂的树模型，再根据交叉验证方法来估计不同剪枝条件下各模型的误差，选择误差最小的树模型；
    - **3.输出最终结果，进行预测和解释**

#### 1.3.4.2 CART节点分裂准则-Gini指数

* **基尼值(Gini value)**
    - 数据集的纯度可用基尼值来度量；
    - 基尼值越小，则数据集的纯度越高；
* **基尼指数(Gini index)**
    - CART决策树[Breiamn et al.,1984]使用**基尼指数(Gini index)**来选择划分变量，CART是二分叉树；
    - 基尼指数越小，则使用相应变量来进行划分所获得的“纯度提升”越大；

对于上面的样本集合 $D$ 的纯度可用基尼值来度量：

$$Gini(D)=\sum^{|\mathcal{Y}|}_{k=1}\sum_{k' \neq k}p_{k}p_{k'}=1-\sum^{|\mathcal{Y}|}_{k=1}p_{k}^{2}$$

假设变量 $a$ 有 $V$ 个可能的取值 $\{a^{1}, a^{2}, \ldots, a^{V}\}$。若使用变量 $a$ 对样本集合 $D$ 进行划分，

* $a$ 为离散变量
    - 对于每一个变量 $a$ 的取值 $a^{i}, (i=1, 2,\ldots,V)$，会产生两个个分支节点。其中第一个分支节点(左分支)包含了 $D$ 中所有在变量 $a$ 上取值为 $a^{i}$ 的样本，记为 $D^{l}$；第二个分支节点(右分支)包含了 $D$ 中所有在变量 $a$ 上取值不为 $a^{i}$ 的样本，记为 $D^{r}$。 
* $a$ 为连续变量
    - 对于每一个变量 $a$ 的取值 $a^{i}, (i=1, 2,\ldots,n)$ (已经从小到大进行了排列：$a^{1} \leqslant a^{2} \leqslant \ldots \leqslant a^{n}$)，基于分割点 $t$ 可将样本集合 $D$ 划分为两个子集，即会产生两个分支节点。其中第一个分支节点(左分支)包含了 $D$ 中所有在变量 $a$ 上取值为 $a \leqslant t$ 的样本，记为 $D^{l}_{t}$；第二个分支节点(右分支)包含了 $D$ 中所有在变量 $a$ 上取值为 $a > t$ 的样本，记为 $D^{r}_{t}$。 显然对于相邻的两个取值 $a^{i}$ 与 $a^{i+1}$来说，$t$ 在区间 $[a^{i},a^{i+1})$ 中取任意值所产生的划分结果都相同。因此，对连续变量，可考察包含 $n-1$ 个元素的候选分割点集合：
    $$T_{a}=\bigg\{\frac{a^{i}+a^{i+1}}{2}|1 \leqslant i \leqslant n-1 \bigg\}$$
即把区间 $[a^{i},a^{i+1})$ 的中位点作为候选分割点，然后就可以像离散变量一样来考察这些分割点，选取最优的分割点进行样本集合的划分。

易知集合 $D^{l}$ 的基尼值为 $Gini(D^{l})$ ，集合 $D^{r}$ 的基尼值为 $Gini(D^{r})$。再考虑到两个不同的分支节点所包含的样本数不同，所以给左右两个分支节点分别赋权重 $\frac{|D^{l}|}{|D|}$，$\frac{|D^{r}|}{|D|}$，即样本数越多的分支节点影响越大，于是可计算出用变量 $a$ 进行划分的基尼指数为：

$$Gini\_index(D,a)=\frac{|D^{l}|}{|D|}Gini(D^{l})+\frac{|D^{r}|}{|D|}Gini(D^{r})$$

选择使得划分后基尼指最数小的变量作为最优划分变量：

 $$a_{*}=\underset{a \in A}{argmin}Gini\_index(D, a)$$

## 1.4 决策树剪枝

剪枝(Purning)是决策树减轻“过拟合”的主要手段。在决策树学习中，为了尽可能地正确分类训练样本，节点划分不断重复，有时会造成决策树分支过多，这时可能因为训练样本学得太好了，以至于把训练样本自身的一些特点当做所有数据都具有的一般性质而导致过拟合。因此可通过去掉一些分支来降低过拟合的风险。

**决策树兼职的基本策略：**

* 预剪枝(prepruning)
    - 在决策树生成过程中，对每个节点在分割前先进行估计。若当前的分割不能带来决策树泛化性能的提升，则停止分割并将当前节点标记为叶节点
    - 在预剪枝过程中，决策树的很多分支都没有展开，这不仅降低了过拟合的风险，还显著减少了决策树的训练时间和预测时间；但有些分支的当前分割虽不能提升泛化性能、甚至能导致泛化性能下降，但在其基础上的后续分割却有可能导致泛化性能的显著提高，容易导致欠拟合
* 后剪枝(postpurning)
    - 先从训练样本集生成一棵最大规模的完整的决策树，然后自底向上地对非叶结点进行考察。若将该提升决策树的泛化性能，则将该子树替换为叶节点节点对应的子树替换为叶节点能
    - 后剪枝决策树比预剪枝决策树保留了更多的分支。后剪枝过程的欠拟合风险很小，泛化性能往往优于预剪枝决策树，但后剪枝过程是在生成完全决策树之后进行的，并且要自底向上地对树中所有的非叶节点进行逐一考察，因此训练时间开销比未剪枝决策树和预剪枝决策树都要大的多

## 1.5 决策树模型优缺点

**决策树模型是一种简单易用的非参数分类器，算法的优缺点：**

* **决策树模型的优点是：**
    - 不需要对数据有任何的先验设计
    - 对噪声数据和缺失数据不敏感
    - 计算速度较快
    - 结果容易解释
    - 稳健性强

* **决策树模型的缺点是：**
    - 1.精确度不高。因为它是矩形的判别边界，使得精确度不高，对回归问题不太适合。(在处理回归问题时建议使用模型树(model tree)方法，即先将数据切分，再对各组数据进行线性回归。`party`包`mod`, `RWeka`包`M5P`可以实现)
    - 2.单个决策树不太稳定。数据微小的变化会造成模型结构变化。
    - 3.有变量选择偏向，即会选择那些有取值较多的变量。(一种改善的方法是使用条件推断树，即`party`包中的`ctree`函数，还可以采用集成学习方法)
    - 4.分类规则复杂。决策树算法在产生规则的时候采用了拒局部贪婪方法，每次只选取一个属性进行分析构造决策树，所以他产生的规则很复杂，针对此问题一般采用剪枝的方法实现。
    - 5.收敛到非全局的局部最优解(ID3)。
    - 6.容易过度拟合。

## 1.6 决策树在`sklearn`中的实现简单用法

### 1.6.1 分类

```python
from sklearn import tree

# data
X = [[0, 0], 
     [1, 1]]
Y = [0, 1]

# train model
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)

# predict
clf.predict([[2.0, 2.0]])

clf.preidct_proba([[2.0, 2.0]])
```

```python
from sklearn.datasets import load_iris
from sklearn import tree

# data
iris = load_iris()

# train model
clf = tree.DecisionTreeClassifier()
clf = clf.fit(iris.data, iris.target)


# export tree in Graphviz format
import graphviz
dot_data = tree.export_graphviz(clf, out_file = None)
graph = graphviz.Source(dot_data)
graph.render("iris")



dot_data = tree.export_graphviz(clf, 
                                out_file = None, 
                                feature_names = iris.feature_names, 
                                class_names = iris.target_names,
                                filled = True, 
                                rounded = True,
                                special_characters = True)
graph = graphviz.Source(dot_data)
graph
```

### 1.6.2 回归

```python
from sklearn import tree

X = [[0, 0], [2, 2]]
y = [0.5, 0.25]

clf = tree.DecisionTreeRegressor()
clf = clf.fit(X, y)
clf.preidct()
```

# 2.AdaBoost

## 2.1 集成学习(ensemble learning)

* 所谓集成学习，是指构建多个分类器（弱分类器）对数据集进行预测，然后用某种策略将多个分类器预测的结果集成起来，作为最终预测结果。通俗比喻就是“三个臭皮匠赛过诸葛亮”，或一个公司董事会上的各董事投票决策，它要求每个弱分类器具备一定的“准确性”，分类器之间具备“差异性”
* 集成学习根据各个弱分类器之间有无依赖关系，分为 Boosting 和 Bagging 两大流派：
	- Boosting流派，各分类器之间有依赖关系，必须串行，比如Adaboost、GBDT(Gradient Boosting Decision Tree)、Xgboost；
	-  Bagging流派，各分类器之间没有依赖关系，可各自并行，比如随机森林(Random Forest)；


## 2.2 AdaBoost模型

Adaboost是boosting流派中最具代表性的一种方法。AdaBoost，是英文"Adaptive Boosting"（自适应增强）的缩写，由Yoav Freund和Robert Schapire在1995年提出。它的自适应在于：前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。

最初提出的 AdaBoost 算法主要用来解决二分类问题. 

* 首先, 初始化所有的训练数据样本的权重为 $\omega_{i}=1/N, i=1, 2, \ldots, N$, 并利用这个加权重的训练数据训练产生一个弱分类器. 
* 然后, 在算法第 $m$ 步迭代中, $m=2, 3, \ldots, M$,算法每次都会根据前一步训练出的弱分类器在训练数据上的分类结果重新计算训练样本权重, 并在每次拟合之前将权重 $\omega_{1}, \omega_{2}, \ldots, \omega_{N}$ 作用在每个训练数据观测值 $(\mathbf{x}_{i}, y_{i}), i = 1, 2, \ldots, N$ 上, 然后不断将弱分类算法应用在这些加权之后的训练数据上, 重新进行拟合分类. 那些在前一步迭代中被弱分类器 $G_{m-1}(\mathbf{x})$ 误分类的的观测值的权重将会增大, 而那些被正确分类的观测值的权重将减小. 因此, 随着迭代过程的进行, 那些很难正确分类的观测值受到的影响也越来越大, 即样本权重越来越大. 因此在序列中每个分类器将被迫重点关注这些很难被之前分类器正确分类的训练数据观测值. 
* 最后, 再将这些弱分类器的分类结果进行加权组合, 得到最终的强分类器.

当然, AdaBoost 算法同样可以用来解决响应变量为连续的回归问题 许多学者都研究了AdaBoost 产生能够准确分类的分类器的原因, 学者们在数据实验中发现, 当使用基于决策树的分类器作为"基本学习器"(base learner) $G_{m}(\mathbf{x}), m=1, 2, \ldots, M$ 时会使得 AdaBoost 算法比单棵决策树分类模型拥有显著低的分类误差. 事实上, Breiman就直接将使用树模型作为基分类器的AdaBoost 算法称为"世界上最好的直接可以拿来使用的分类器"(best off-the-shelf classifier in the world). 并且, 许多关AdaBoost 的算法实验都表明如果算法中不断有基本学习器加进来, 算法的分类误差一直在减小, 从而可以得到AdaBoost 算法似乎不容易过拟合的性质.


**Adaboost 迭代算法分3步：**

1. 初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N；
2. 训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去；
3. 将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小；

**AdaBoost具体算法：**

> 1. 初始化每个样本观测值的权重 $\omega_{i}^{[0]}=1/N$ , 其中 $i=1, \ldots, N$；
> 2. 开始迭代, 令 $m=1$
	- 利用加权重 $\omega_{i}^{[m-1]}$ 的训练数据拟合一个弱分类器 $G_{m}(\cdot)$；
	- 计算加权训练数据的分类错误率:
 $$err^{[m]} = \frac{\sum^{N}_{i=1}\omega_{i}^{[m-1]}I(y_{i} \neq G_{m}(\mathbf{x}_{i}))}{\sum^{N}_{i=1}\omega_{i}^{[m-1]}}.$$
	- 根据上一步的分类错误率计算分类器在最终分类结果上的权重值:
 $$\alpha^{[m]}=log(\frac{1-err^{[m]}}{err^{[m]}}).$$
	- 根据上面计算出的样本的误分类率更新样本的权重:
 $$\omega_{i}^{[m]} \leftarrow \omega^{[m-1]}_{i}exp(\alpha^{[m]}I(y_{i} \neq G_{m}(\mathbf{x}_{i}))), $$
* 重复上面的迭代，直到满足迭代停止条件 $m=M$, 并且将所有在迭代中产生的弱分类器通过加权投票的方法进行聚合, 最终得到的强分类器为:
 $$G(\mathbf{x}) = sign\Bigg(\sum^{M}_{m=1}\alpha^{[m]}G^{[m]}(\mathbf{x})\Bigg)$$



# 3.GBM


## 3.1 GBM模型原理

> 随着 Breiman 对 AdaBoost 算法的突破性理解和解释的提出, Friedman, Hastie 和 Tibshirani 将提升算法解释为在统计学框架下的拟合累加模型的函数估计问题, 并且, Friedman 将提升算法扩展为一种利用类似最速下降法优化方法, 在具体的损失函数梯度方向上逐步拟合基本学习器的梯度提升器算法(Gradient Boosting Machine), 也称为 梯度提升模型算法(Gradient Boosting Modeling). 从而将提升算法扩展到许多应用上, 同时也产生了一系列具体的梯度提升算法, 例如, 对于回归问题, 利用损失函数为平方误差损失函数 $L(y, f) = (y-f)^{2}$ 时产生的 $L_{2}$Boost 算法; 对于分类问题, 应用对数似然损失函数 $L(y, f) = log(1+e^{-yf})$ 得到了 LogitBoost 算法, 而选择指数损失函数 $L(y, f)=exp(-yf)$, 将会得到前一节介绍的AdaBoost 算法.


### 3.1.1 函数估计问题


假设在一个函数估计问题中, 存在一个随机响应变量 $y$ 和一组随机解释变量 $\mathbf{x}=\{x_{1}, \ldots, x_{d}\}$, 其中 $d$ 是解释变量的个数. 现在, 给定训练数据集 $\{({\mathbf{x}}_{i}, y_{i}), i=1, 2, \ldots, N\}$ 为变量 $(\mathbf{x}, y)$ 的观测值. 函数估计问题的目标就是利用解释变量 $\mathbf{x}$ 和响应变量 $y$ 观测值的联合分布通过最小化一个特殊的损失函数 $L(y, f(\mathbf{x}))$ 的期望值得到一个估计或近似函数 $f(\mathbf{x})$, 函数 $f(\mathbf{x})$ 的作用就是将解释变量 $\mathbf{x}$ 映射到响应变量 $y$.

$$f^{*}=\arg\underset{f}{\min}E_{y, \mathbf{x}}[L(y, f(\mathbf{x}))]$$

其中: 

* 损失函数 $L(y, f(\mathbf{x}))$ 是为了评估响应变量与其函数估计值的接近程度.
* 实际应用中存在很多常用的损失函数, 其中有用来解决回归问题的平方误差损失函数 $L(y, f)=(y-f)^{2}$ 和绝对误差损失函数 $L(y, f)=|y-f|$, 其中~$y \in \mathbf{R}$; 还有用来解决二分类问题的负二项对数似然损失 $L(y, f)=log(1+e^{-2yf})$, 其中 $y \in \{-1, 1\}$.


### 3.1.2 梯度提升模型(GBM)

梯度提升模型(GBM)是一种对"累加"("additive") 扩展模型的拟合方法, 在这里,
"累加"(additive) 扩展模型是指由一簇"基本函数"(base function) 扩展成的函数空间中的函数组合. 而这里的"基本函数"相当于在基分类器 $G_{m}(\mathbf{x}) \in \{-1, 1\}$. 因此方程中关于响应变量 $y$ 的估计函数 $f(\mathbf{x})$ 可以表示为一种参数化的"累加"扩展形式

$$f(\mathbf{x}; \{\beta_{m},\gamma_{m}\}_{1}^{M}) = \sum_{m=1}^{M}\beta_{m}b(\mathbf{x};\gamma_{m}).$$

其中: 

* $\{\beta_{m}, \gamma_{m}\}_{1}^{M}$ 是估计函数$f(\cdot)$ 的参数集合. 并且, 函数 $b(\mathbf{x};\gamma_{m}), m=1, 2, \ldots, M$ 通常是关于解释变量 $\mathbf{x}$ 的简单学习器函数, 例如 $b(\mathbf{x}; \gamma_{m})$ 可以是一个简单的回归树函数, 其中参数 $\gamma_{m}$ 是回归树中的分裂变量及分裂位置值.

给定训练数据 $\{(\mathbf{x}_{i}, y_{i}), i=1,2,\ldots, N\}$, 将上面的累加模型代入函数估计问题中有

$$\underset{\{\beta_{m}, \gamma_{m}\}^{M}_{m=1}}{\min}E_{y,\mathbf{x}}(L(y_{i}, \sum_{m=1}^{M}\beta_{m}b(\mathbf{x}_{i};\gamma_{m}))), $$

即

$$ \underset{\{\beta_{m}, \gamma_{m}\}^{M}_{m=1}}{\min}\sum_{i=1}^{N}L(y_{i}, \sum_{m=1}^{M}\beta_{m}b(\mathbf{x}_{i};\gamma_{m})),$$

因此, 方程中的函数估计问题就变成了一个参数估计问题.

在梯度提升模型(GBM) 中, 对于上面的估计问题作者希望利用前向分步累加模型(Forward Stagewise Additive Modeling) 算法进行求解, 前项分步累加模型算法如下

> 1. 初始化~$f_{0}(\mathbf{x})=0$.
> 2. 进行迭代, $m=1, 2, \ldots, M$
    - 计算
$$(\beta_{m}, \gamma_{m})=\underset{\beta, \gamma}{\arg\min}\sum_{i=1}^{N}L(y_{i}, f_{m-1}(\mathbf{x}_{i})+\beta b(\mathbf{x}_{i}; \gamma)).$$
> 3. 更新估计函数
$$f_{m}(\mathbf{x})=f_{m-1}(\mathbf{x})+\beta_{m}b(\mathbf{x};\gamma_{m}).$$

在机器学习中, 上面的方程被称为提升(boosting), 函数 $b(\mathbf{x};\gamma)$ 被称为弱分类器(weak learner) 或者基本学习器(base learner), 并且一般是一个分类树.

然而, 对于具体的损失函数 $L(y, f(\mathbf{x}))$ 和 基本学习器函数 $b(\mathbf{x}; \gamma)$, 前向分步累加模型很难得到最优解. 作者在这里采用了一种类似最速下降法来解决前向分步累加模型算法中的估计问题. 因为在前向分步累加模型的方程中, 如果给定估计函数 $f_{m-1}(\mathbf{x})$, 则 $\beta_{m}b(\mathbf{x};\gamma_{m})$ 可以看成是最速下降算法中求解最优解 $f^{*}_{M}(\mathbf{x})$ 的最优的贪婪迭代项. 因此, 应用最速下降法, 将估计函数 $f(\mathbf{x})$ 的数值最解~$f_{M}^{*}(\mathbf{x})$ 表示为下面的形式

$$f_{M}^{*}(\mathbf{x})=\sum^{M}_{m=0}h_{m}(\mathbf{x}),$$

其中:

* $f_{0}(\mathbf{x})=h_{0}(\mathbf{x})$ 是一个初始化的猜测值, $h_{m}(\mathbf{x}), m=1, 2, \ldots, M$ 是最速下降算法中定义的连续增量函数. 最速下降法定义上面的增量函数 $h_{m}(\mathbf{x}), m=1, 2, \ldots, M$ 如下所示

$$h_{m}(\mathbf{x})=-\eta _{m}g_{m}(\mathbf{x}),$$

其中:

$$g_m(\mathbf{x}) = \Bigg[\frac{\partial E_{y, \mathbf{x}}[L(y, f(\mathbf{x}))]}{\partial f(\mathbf{x})}\Bigg]_{f(\mathbf{x})=f_{m-1}(\mathbf{x})} \\
= E_{y, \mathbf{x}}\Bigg[\frac{\partial L(y, f(\mathbf{x}))}{\partial f(\mathbf{x})}\Bigg]_{f(\mathbf{x})=f_{m-1}(\mathbf{x})},$$

其中:

$g_{m}(\mathbf{x}) \in R^{N}$ 为损失函数 $L(y, f(\mathbf{x}))$ 在 $f(\mathbf{x})=f_{m-1}(\mathbf{x})$ 处的梯度向量. 并且

$$f_{m-1}(\mathbf{x})=\sum^{m-1}_{i=0}f_{i}(\mathbf{x}),$$

步长: $\eta_{m}, m=1,2,\ldots, M$ 可以通过线性搜索算法得到

$$\eta_{m}=\arg\underset{\eta}{\min}E_{y,\mathbf{x}}L(y_{i}, f_{m-1}(\mathbf{x})-\eta g_{m}(\mathbf{x})).$$

上面的过程重复迭代, 直到满足算法设定的停止条件.
此时最速下降算法的函数更新形式为

$$f_{m}(\mathbf{x}) = f_{m-1}(\mathbf{x})-\eta g_{m}(\mathbf{x}).$$

可以看出, 最速下降法是一种十分贪婪的数值优化策略, 因为算法中负梯度$g_{m}$ 是函数空间$R^{N}$ 中, 损失函数$L(y, f)$ ~$f=f_{m-1}$ 处下降最快的局部方向.

如果在训练数据上最小化损失函数$L(y,f(\mathbf{x}))$ 是最终的目标, 那么最速下降法会是一种很好的解决方法. 因为$g_{m}(\mathbf{x})$ 对于任何可导的损失函数$L(y, f(\mathbf{x}))$ 都是比较容易求得的. 然而, 最速下降法中计算得到的$g_{m}(\mathbf{x})$ 只是在训练数据上定义的, 而梯度提升算法的目的却是将最终的模型泛化到除了训练数据之外的未知数据上. 这样训练出的梯度提升算法才具有泛化能力.

因此, 梯度提升模型算法通过利用一个基本的学习器模型算法将负梯度向量进行拟合, 得到了负梯度向量值$-g_{m}(\mathbf{x}_{i}), i=1,2,\ldots, N$ 的近似估计向量, 即和产生了一个在前项分步模型算法中的基本学习器. 然后将这个近似估计向量应用在最速下降算法中代替负梯度向量, 从而使得提升算法拥有泛化能力. 下面是利用平方误差损失拟合负梯度向量估计值的表达式

$$a_{m}=\arg\underset{a}{\min}\sum_{i=1}^{N}[-g_{m}(\mathbf{x}_{i})-b(\mathbf{x}_{i}; a)]^{2},$$

当然实际应用中也可以使用其他的一些基本学习器模型来进行拟合. 比较常用的有决策树模型拟合一棵树模型. 而基学习器的权重系数仍然使用最速下降算法中的线性搜索算法得到

$$\eta_{m}=\arg\underset{\eta}{\min}\sum_{i=1}^{N}L(y_{i}, f_{m-1}(\mathbf{x}_{i})+\eta b(\mathbf{x}_{i}; a_{m}))$$

然后, 将估计近似函数进行更新

$$f_{m}(\mathbf{x})=f_{m-1}(\mathbf{x})+\eta_{m}b(\mathbf{x}, a_{m})$$

一般的梯度提升算法的伪代码如下:

> 1. 初始化~$\hat{f}^{[0]}(\cdot)$ 为一个常数值. 通常的选择为:
$$\hat{f}^{[0]}(\cdot) \equiv \arg \underset{c}{\min}\frac{1}{N}\sum_{i=1}^{n}L(y_{i}, c),$$
或者为 $\hat{f}^{[0]} \equiv 0$, 令 $m=0$;
> 2. 增加 $m=1$. 计算负梯度 $-\frac{\partial}{\partial f}$ 并且计算负梯度在 $\hat{f}^{[m-1]}(\mathbf{x}_{i})$ 处的值:
$$U_{i}=-\frac{\partial}{\partial f}L(y_{i}, f)\Bigg|_{f=\hat{f}^{[m-1]}(\mathbf{x}_{i})}, i=1, \ldots, N;$$
> 3. 将负梯度向量~$U_{1}, \ldots, U_{N}$ 通过一个基本的学习模型(例如: 回归) 拟合到预测变量的观测值向量 $\mathbf{x}_{1}, \ldots, \mathbf{x}_{N}$:
$$(\mathbf{x}_{i}, U_{i})^{N}_{i=1} \rightarrow \hat{b}^{[m]}(\cdot);$$
> 4. 更新估计函数:
$$\hat{f}^{[m]}(\cdot)=\hat{f}^{[m-1]}(\cdot)+\eta_{m} \cdot \hat{b}^{[m]}(\cdot),$$
其中$0 \leqslant \eta_{m} \leqslant 1$ 是一个步长因子;
> 5. 重复第二步到第四步直到 $m=M$;
> 6. 输出训练得到的学习器 $f^{[M]}(\cdot)$

### 3.1.3 梯度提升模型算法的应用

在上一节, 我们已经给出了梯度提升模型算法的详细推导及其一般性算法伪代码. 可以看出, 在梯度提升算法中, 算法在第2 步中对一个具体的损失函数 $L(y, f(\mathbf{x}))$ 求负梯度向量 $U_{i}, i=1,2,\ldots, N$, 而在第 3 步则利用一个具体的基本学习器模型算法, 对数据预测变量观测值和负梯度向量 $U_{i}, i=1,2,\ldots, N$ 进行拟合产生负梯度向量 $U_{i}, i=1,2,\ldots, N$ 的近似估计. 因此, 在梯度提升模型算法中, 应用不同的损失函数 $L(y,f(\mathbf{x}))$ 和不同的基本学习器模型算法可以得到不同的提升算法模型.

对于损失函数的选择, 只要损失函数 $L(\cdot, \cdot)$ 满足对于它的第二个参数变量是光滑且凸的, 就可以应用到梯度提升算法的第2 步中. 这一小节, 具体讨论梯度提升算法的一些特殊应用算法. 包括应用平方误差损失函数的 $L_{2}$Boost 算法, 应用负二项对数似然损失的 LogitBoost 算法 以及基于分位数回归模型的分位数提升分类(QBC) 算法. 

下面是这些损失函数的形式

* 用在$L_{2}$Boost 算法中的平方误差损失函数
$$L(y, f) = (y - f)^{2}/2,$$
* 用在 LogitBoost 算法中的对数似然损失函数
$$L(y, f) = log_{2}(1+exp(-2yf)),$$
* 用在分位数提升分类~(QBC) 算法中基于分位数回归模型产生的损失函数
$$L(y, f) = [y-(1-\tau)]K(f/h)$$
	- 其中: $K(\cdot)$ 是一个标准正态分布的累积分布函数, h是一个给定的大于零的常数.




## 3.2 GBM模型调参(Python)

### 3.2.1 参数类型

* 决策树参数
	- `min_samples_split`
		- 要分裂的树节点需要的最小样本数量，若低于某个阈值，则在此节点不分裂；
		- 用于控制过拟合，过高会阻止模型学习，并导致欠拟合；
		- 需要使用CV进行调参；
	- `min_samples_leaf`
		- 叶子节点中所需的最小样本数，若低于某个阈值，则此节点的父节点将不分裂，此节点的父节点作为叶子结点；
		- 用于控制过拟合，同`min_samples_split`；
		- 一般选择一个较小的值用来解决不平衡类型样本问题；
	- `min_weight_fraction_leaf`
		- 类似于`min_sample_leaf`；
		- 一般不进行设置，上面的两个参数设置就可以了；
	- `max_depth`
		- 一棵树的最大深度；
		- 用于控制过拟合，过大会导致模型比较复杂，容易出现过拟合；
		- 需要使用CV进行调参；
	- `max_leaf_nodes`
		- 一棵树的最大叶子节点数量；
		- 一般不进行设置，设置`max_depth`就可以了；
	- `max_features`
		- 在树的某个节点进行分裂时的考虑的最大的特征个数，一般进行随机选择，较高的值越容易出现过拟合，但也取决于具体的情况；
		- 一般取特征个数的平方根(跟随机森林的选择一样)；
* Boosting参数
	- `learning_rate`
		- 每棵树对
	- `n_estimators`
	- `subsample`
		- 构建每棵数时选择的样本数；
* 其他参数
	- `loss`
	- `init`
	- `random_state`
	- `verbose`
	- `warm_start`
	- `presort`


### 3.2.2 调参策略

* 一般参数调节策略：
	- 选择一个相对来说较高的learning rate，先选择默认值0.1(0.05-0.2)
	- 选择一个对于这个learning rate最优的树的数量(合适的数量为：40-70)
		- 若选出的树的数量较小，可以减小learning rate 重新跑GridSearchCV
		- 若选出的树的数量较大，可以增加初始learning rate 重新跑GridSearchCV
	- 调节基于树的参数
	- 降低learning rate，增加学习期的个数得到更稳健的模型
* 对于learning rate的调节，对其他树参数设置一些默认的值
	- min_samples_split = 500
		- 0.5-1% of total samples
		- 不平衡数据选择一个较小值
	- min_samples_leaf = 50
		- 凭感觉选择，考虑不平衡数据，选择一个较小值
	- max_depth = 8
		- 基于数据的行数和列数选择，5-8
	- mat_features = 'sqrt'
	- subsample = 0.8
* 调节树参数
	- 调节`max_depth`，`min_samples_split`
	- 调节`min_samples_leaf`
	- 调节`max_features`



# 4.XGBoost

## 4.1 XGBoost模型

**XGBoost特点：**

* 传统的GBDT以CART作为基函数(base learner,base classifier,base function)，XGBoost还支持线性分类器 (linear classifier, linear regression, logistic regression)
* 传统GBDT在优化时只用到一阶导数信息（负梯度），xgboost则对代价函数进行了二阶泰勒展开，同时用到一阶和二阶导数。且xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导
* 基于预排序(pre_sorted)方法构建决策树
	- 优点：精确地找到分割点
	- 缺点：内存消耗大，时间消耗大，对cache优化不友好
	- 首先，对所有特征都按照特征的数值进行排序；
	- 其次，在遍历分割点的时候用O(#data)的代价找到一个特征上的最好分割点；
	- 最后找到一个特征的分割点后，将数据分裂成左右子节点；

**XGBoost优势：**

* 正则化 GBM (Regularization)
	- 控制模型过拟合问题；
* 并行 GBM (Parallel Processing)
* 高度灵活性 (High Flexibility)
* 能够处理缺失数据
* 自带树剪枝
* 内置交叉验证
* 能够继续使用当前训练的模型



* 可扩展性强
* 为稀疏数据设计的决策树训练方法
* 理论上得到验证的加权分位数粗略图法
* 并行和分布式计算
* 设计高效核外计算，进行cache-aware数据块处理

**XGBoost缺点（LightGBM的出发点）：**

* 每轮迭代时，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。
* 预排序方法（pre-sorted）：首先，空间消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如排序后的索引，为了后续快速的计算分割点），这里需要消耗训练数据两倍的内存。其次时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。
* 对cache优化不友好。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的cache miss。



## 4.2 XGBoost模型理论推导


### **模型目标函数**

$$L(\phi)=\sum_{i=1}^{n} l(y_i, \hat{y_i}) + \sum_{k=1}^{K} \Omega(f_k)$$

其中：

* $L(\cdot)$：是目标函数；
* $l(\cdot)$：是损失函数，通常是凸函数；用于刻画预测值 $\hat{y_i}$ 和真实值 $y_i$之间的差异
* $\Omega(\cdot)$：是模型的正则项；用于降低模型的复杂度，减轻过拟合问题
	- 决策树叶子节点数量，树深度
	- 决策树叶节点权重得分的L1, L2正则
* $f_k$：第 $k$ 棵树
* $y_i$：目标变量
* $\hat{y_i}=\sum_{k=1}^{K}f_k(x_i), f_k \in F$
	- 回归: 预测得分
	- 分类：预测概率
	- 排序：排序得分


### **模型目标**

$$\min L(\phi)$$


### **模型损失函数定义**

根据Gradient Boosting思想，假设第 $t$ 轮迭代的目标函数：

$$
L^{(t)} = \sum_{i=1}^{n} l \bigg(y_i, \hat{y_i}^{(t-1)} + f_t(x_i) \bigg) + \Omega(f_t) + constant
$$


对目标函数 $L^{(t)}$ 在 $\hat{y_i}^{(t-1)}$ 处进行二阶泰勒展开，可以加速优化过程，得到目标函数的近似：

$$
L^{(t)} \simeq \sum_{i=1}^{n} 
[
l(y_i, \hat{y_i}^{(t-1)}) + 
g_i f_t (x_i) + 
\frac{1}{2} h_i f_t^2 (x_i)]+ \Omega(f_t) + constant
$$ 


> * 函数$f(x)$在$x_0$处的二阶泰勒展开式：
> $$f(x) = f(x_0) + f' (x_0)(x - x_0) + f''(x_0)(x - x_0)^2$$
> * 目标函数 $l(y_i, x)$ 在 $\hat{y_i}^{(t-1)}$ 处的二阶泰勒展开式：
> $$l(y_i, x) = l(y_i, \hat{y_i}^{(t - 1)}) + \frac{\partial l(y_i, \hat{y_i}^{(t - 1)})}{\partial \hat{y_i}^{(t - 1)}} (x - \hat{y_i}^{(t - 1)}) + \frac{1}{2} \frac{{\partial ^2} l(y_i, \hat{y_i}^{(t - 1)}) } {\partial \hat{y_i}^{(t - 1)}} (x - \hat{y_i}^{(t - 1)})^2$$
> * 令 $x= \hat{y_i}^{(t-1)} + f_t (x_i)$, 记一阶导数为 $g_i = \frac{\partial l(y_i, \hat{y_i}^{(t - 1)})}{\partial \hat{y_i}^{(t - 1)}}$， 记二阶导数为 $h_i = \frac{{\partial ^2} l(y_i, \hat{y_i}^{(t - 1)})}{\partial \hat{y_i}^{(t - 1)}}$；可以得到
> $$l(y_i, \hat{y}^{(t-1)} + f_t(x_i)) = l(y_i, \hat{y}^{(t - 1)}) + g_i f_t(x_i) + \frac{1}{2} h_i f_t^2 (x_i)$$


上面的目标函数 $L^{(t)}$ 中，第一项 $l(y_i, \hat{y_i}^{(t-1)})$ 以及 $constant$ 为常数项，在优化问题中可直接删除：

$${\tilde L}^{(t)} = \sum_{i=1}^{n} [g_{i}f_{t}(x_i) + \frac{1}{2}h_{i}f_{t}^{2}(x_i)]  + \Omega (f_t)$$


### **模型正则项定义**


通过`叶子节点的权重得分`和`叶子节点的索引值(结构)`定义一棵树：

$$f_t(x) = \omega_{q(x)}, \omega \in R^T, q: R^d \rightarrow \{1, 2, \ldots, T\}$$

其中：

* $\omega$: 叶子节点权重得分向量
* $q(\cdot)$: 树结构

定义正则项(可以使其他形式)：

$$\Omega(f_t)=\gamma T + \frac{1}{2}\lambda\sum_{j=1}^{T}\omega_j^2$$

其中：

* $T$ 是 叶节点的个数
* $\omega_j^2$是叶子节点权重得分的L2范数


记决策树每个叶节点上的样本集合为：

$${I_j} = \{ {i | q(x_i) = j} \}$$


将正则项 $\Omega(f_t)$ 展开：

$$\eqalign{
& {\tilde L}^{(t)} 
= \sum_{i=1}^{n}[g_i f_t(x_i) + \frac{1}{2}h_i f_{t}^{2}(X_I)] + \Omega(f_t) \cr 
& \;\;\;\;\;{\rm{ 
= }} \sum_{i=1}^{n}[g_i \omega_{q(x_i)} + \frac{1}{2} h_i \omega_{q(x_i)}^{2}] + \gamma T + \frac{1}{2} \lambda \sum_{j = 1}^{T} w_j^2   \cr 
& \;\;\;\;\;{\rm{ 
= }} \sum_{j = 1}^{T}[(\sum_{i \in I_j} g_i ) w_j + \frac{1}{2} (\sum_{i \in {I_j}} h_i  + \lambda ) w_j^2 ]  + \gamma T \cr}$$

记 $G_j=\sum_{i\in I_j}g_i$， $H_j=\sum_{i\in I_j}h_i$

$$\eqalign{
& {\tilde L}^(t) 
= \sum{j = 1}^{T} [(\sum_{i \in {I_j}} g_i) w_j + \frac{1}{2} (\sum_{i \in {I_j}} h_i  + \lambda ) w_j^2 ]  + \gamma T \cr
& \;\;\;\;\;{\rm{ 
= }} \sum_{j = 1}^{T} [G_j w_j + \frac{1}{2} (H_j  + \lambda ) w_j^2 ]  + \gamma T \cr
}$$


### **模型目标函数求解**

对于固定的树结构 $q(x)$，对 $\omega_j$ 求导等于0，得到目标函数的解析解$\omega_{j}^{\star}$：

$$w_{j}^{\star} = \frac{\sum\limits_{i \in I_j} g_i}{\sum\limits_{i \in I_j} h_i + \lambda}$$

即

$$w_{j}^{\star} = -\frac{G_j}{H_j+\lambda} $$

将上面得到的解析解带入目标函数：

$${{\tilde L}^{( t )}}=-\frac{1}{2}\sum_{j=1}^{T}\frac{G_j^2}{H_j+\lambda} + \gamma T$$

> 这里的 ${{\tilde L}^{( t )}}$ 代表了当指定一个树结构时，在目标函数上最多减少多少，这里叫做`结构分数(structure score)`；这个分数越小，代表这个树的结构越好；



### 模型中决策树节点分裂算法


#### 贪心算法(Exact Greedy Algorithm for split finding)

每次尝试对已有叶节点进行一次分割，分割的规则：


$$Gain = \frac{1}{2}\Bigg[\frac{G_{L}^{2}}{H_K + \lambda} + \frac{G_{R}^{2}}{H_R + \lambda} - \frac{(G_L + G_R)^{2}}{H_L + H_R + \lambda}\Bigg] - \gamma$$


其中：

* $\frac{G_{L}^{2}}{H_K + \lambda}$：左子树分数；
* $\frac{G_{R}^{2}}{H_R + \lambda}$：右子树分数；
* $\frac{(G_L + G_R)^{2}}{H_L + H_R + \lambda}$： 不分割可以得到的分数；
* $\gamma$：假如新的子节点引入的复杂度代价；


对树的每次扩展，都要枚举所有可能的分割方案；并且对于某次分割，都要计算每个特征值左边和右边的一阶和二阶导数和，从而计算这次分割的增益 $Gain$；
对于上面的分割增益 $Gain$，都要判断分割每次分割对应的 $Gain$ 的大小，并且进行优化，取最小的 $Gain$，直到当新引入一个分割带来的增益小于某个阈值时，就去掉这个分割。这里的优化，相当于对树进行剪枝。


#### 近似算法(Approximate Algorithm for split finding)

> 针对数据量大的情况，不能直接计算


### 自定义损失函数


### XGBoost调参


**参数类型：**

* 通用参数
	- 控制整个模型的通用性能；
	- `booster`：基本学习器类型
		- `gbtree`：基于树的模型
		- `gblinear`：线性模型
	- `silent`
		- 0：打印训练过程中的信息
		- 1：不会打印训练过程中的信息
	- `nthread`：模型并行化使用系统的核数
* Booster参数
	- 控制每步迭代中每个基学习器(树/线性模型)；
	- `eta`：learning rate
		- 0.3
		- 0.01 ~ 0.2
		- 通过shrinking每步迭代的中基本学习器的权重，使模型更加稳健
	- min_child_weight
		- 子节点中所有样本的权重和的最小值
		- 用于控制过拟合
	- `max_depth`
		- 树的最大深度
		- 用于控制过拟合
		- 3 ~ 10
	- `max_leaf_nodes`
		- 树中叶节点的最大数量
	- `gamma`
		- 当分裂结果使得损失函数减少时，才会进行分裂，指定了节点进行分裂所需的最小损失函数量；
	- `max_delta_step`
		- 每棵树的权重估计，不常用
	- `subsample`
		- 定义了构建每棵树使用的样本数量，比较低的值会使模型保守，防止过拟合；太低的值会导致模型欠拟合；
		- 0.5 ~ 1
	- `colsample_bytree`
		- 类似于GBM中的`max_features`，定义了用来构建每棵树时使用的特征数
		- 0.5 ~ 1
	- `colsample_bylevel`
		- 定义了树每次分裂时的特征比例，不常用，用`colsample_bytree`
	- `lambda`
		- 叶节点权重得分的l2正则参数
		- 不常使用，一般用来控制过拟合；
	- `alpha`
		- 叶节点权重得分的l1正则参数
		- 适用于高维数据中，能使得算法加速
	- `scale_pos_weight`
		- 用在高度不平衡数据中，能够使算法快速收敛；
* 学习任务参数
	- 控制模型优化的表现；
	- `objective`
		- `binary:logistic`：Logistic Regression(二分类)，返回分类概率
		- `multi:softmax`：利用`softmax`目标函数的多分类，返回分类标签
			- 需要设置`num_class`
		- `multi:softprob`：类似于`multi:softmax`，返回概率值
	- `eval_metric`
		- `rmse`：平方根误差
		- `mae`：绝对平均误差
		- `logloss`：负对数似然
		- `error`：二分类错误率
		- `merror`：多分类错误率
		- `mlogloss`：多分类负对数似然
		- `auc`： Area Under the Curve


**参数调优的一般策略：**

1. 首先，选择一个相对较大的`learning rate`，比如:0.1 (一般分为在: 0.05-0.3)。根据这个选定的`learning rate`对树的数量`number of tree`进行CV调优；
2. 调节树参数：`max_depth`, `min_child_weight`, `gamma`, `subsample`, `colsample_bytree`；
3. 调节正则化参数`lambda`, `alpha`；
4. 减小`learning rate`，并且优化其他参数；

**参数调优步骤：**

* 选择一个初始化的`learning_rate`和`n_estimators`，利用CV对`n_estimators`进行调优，选择一个最优的`n_estimators`；
	- 对其他模型参数进行初始化(选择一个合理的值)：
		- `max_depth = 5`
			- 3 - 10
		- `min_child_weight = 1`
			- 类别不平衡数据选择一个较小值
		- `gamma = 0`
			- 0.1 - 0.2
		- `subsample = 0.8`
			- 0.5 - 0.9
		- `colsample_bytree = 0.8`
			- 0.5 - 0.9
		- `scale_pos_weight = 1`
			- 类别不平衡数据喜讯则一个较小值
* 调节对模型结果影响最大的参数
	- `max_depth`
	- `min_child_weight`




## 3.3 XGBoost 应用

### 3.3.1 XGBoost Python API

> XGBoost能够读取多种数据格式，libsvm, csv, numpy 2d array, scipy 2d sparse array, pandas dataframe, XGBoost binary buffer
> XGBoost不支持数据中有类别性数据，需要进行One-Hot encoding
> 


### 3.3.2 XGBoost R API



# 5.LightGBM模型理论

原始算法论文：[这里](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf)

强势之处：

* 更快的训练速度
* 更低的内存消耗
* 更好的准确率(泛化性能)
* 支持分布式，可以快速处理海量数据


## 5.1 LightGBM模型算法详解

* 基于Histogram的决策树算法 —— 速度和内存的优化
	- Histogram算法基本思想：
    	- 先把连续的浮点特征值离散化成k个整数，同时构造一个宽度有k的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。
    	- 使用直方图算法有很多优点。首先，最明显就是内存消耗的降低，直方图算法不仅不需要额外存储预排序的结果，而且可以只保存特征离散化后的值，而这个值一般用8位整型存储就足够了，内存消耗可以降低为原来的1/8。其次，在计算上的代价也大幅降低，预排序算法每遍历一个特征值就需要计算一次分裂的增益，而直方图算法只需要计算k次（k可以认为是常数），时间复杂度从O(#data*#feature)优化到O(k*#features)。
    	- Histogram算法并不是完美的。由于特征被离散化后，找到的并不是很精确的分割点，所以会对结果产生影响。但在不同的数据集上的结果表明，离散化的分割点对最终的精度影响并不是很大，甚至有时候会更好一点。原因是决策树本来就是弱模型，分割点是不是精确并不是太重要；较粗的分割点也有正则化的效果，可以有效地防止过拟合；即使单棵树的训练误差比精确分割的算法稍大，但在梯度提升（Gradient Boosting）的框架下没有太大的影响。
* 带深度限制的Leaf-wise的叶子生长策略 ——准确率的优化
* 直方图做差加速 ——  速度和内存的优化
* 直接支持类别型特征(Categorical Feature) ——准确率的优化
* Cache命中率优化 ——内存的优化
* 基于直方图的稀疏特征优化
* 多线程优化
	- 特征并行
	- 数据并行
	- 投票并行
* GPU支持
* 应用
	- 回归(Regression)
	- 二分类(Binary Classification)
	- 多分类(Multi-class Classification)
	- Lambdarank
* 模型评估
	-L1 loss
	-L2 loss
	-Log loss
	-Classification error rate
	-AUC
	-NDCG
	-Multi class log loss
	-Multi class error rate


## 5.2 LightGBM使用方法

**1.安装**

* CLI版本
	- Win
	- Linux
	- OSX
	- Docker
	- Build MPI版本
	- Build GPU版本

* Python library
	- 安装依赖库
```python
!pip install setuptools wheel numpy scipy scikit-learn -U 
!pip install sklearn 
!pip install lightgbm
```
	- 测试安装
```python
import lightgbm as lgb
```
* R package
```r
install.packages("")
```

**2.数据接口（Python）**

* libsvm，tsv，csv文本文件
* numpy 2维数组
* pandas 对象
* LightGBM 二进制文件

用法：

```python
import lightgbm as lgb

# 加载数据
# --------------------------------------------
# 加载文本文件数据或LightGBM二进制文件
# --------------------------------------------
train_csv = lgb.Dataset('train.csv')
train_tsv = lgb.Dataset('train.tsv')
train_svm = lgb.Dataset('train.svm')
train_bin = lgb.Dataset('train.bin')

# --------------------------------------------
# 加载numpy2维数组
# --------------------------------------------
data = np.random.rand(500, 10)
label = np.random.randint(2, size = 500)
train_array = lgb.Dataset(data, label = label)
# --------------------------------------------
# 加载scipy.sparse.csr_matrix数组
# --------------------------------------------
csr = scipy.sparse.csr_matirx((dat, (row, col)))
train_sparse = lgb.Dataset(csr)

# --------------------------------------------
# 保存数据为LightGBM二进制文件
# --------------------------------------------
train_data.save_binary('train.bin')


# --------------------------------------------
# 创建验证数据(validation data，在 LightGBM 中, 验证数据应该与训练数据一致（格式一致）
# --------------------------------------------
test_data1 = train_data.create_vaild('test_svm')
test_data2 = lgb.Dataset('test.svm', reference = train_data)

# --------------------------------------------
# 指定特征名称;
# 指定分类特征(构造Dataset之前应该将分类特征转换为int类型的值);
# 设置权重;
# 初始化score
# 设置group/query数据以用于ranking(排序)任务
# --------------------------------------------
data = np.random.rand(500, 10)
label = np.random.randint(2, size = 500)
train_array = lgb.Dataset(data, label = label)
w = np.random.rand(500, 1)

train_data = lgb.Dataset(data, 
                         label = label, 
                         feature_name = ['c1', 'c2', 'c3'], 
                         categorical_feature = ['c3'],
                         weight = w,
                         free_raw_data = True)
# or
train_data.set_weight(w)
 
train_data.set_init_score()

train_data.set_group()
```

**3.设置参数**

参数设置方式：

* 命令行参数
* 参数配置文件
* python参数字典

参数类型

* 核心参数
* 学习控制参数
* IO参数
* 目标参数
* 度量参数
* 网络参数
* GPU参数
* 模型参数
* 其他参数

```python
param = {
    'num_levels': 31,
    'num_trees': 100,
    'objective': 'binary',
    'metirc': ['auc', 'binary_logloss']
}
```

**4.训练、保存、加载模型**

```python
# 训练模型
num_round = 10
bst = lgb.train(param,
                train_data,
                num_round,
                vaild_sets = [test_data])
# 保存模型
bst.save_model('model.txt')
json_model = bst.dump_model()
# 加载模型
bst = lgb.Booster(model_file = 'model.txt')
```


**5.交叉验证**

```python
num_round = 10
lgb.cv(param, train_data, num_round, nfold = 5)
```

**6.提前停止**

```python
bst = lgb.train(param,
                train_data,
                num_round,
                valid_sets = valid_sets,
                ealy_stooping_rounds = 10)
```

**7.预测**

* 用已经训练好的或加载的保存的模型对数据集进行预测
* 如果在训练过程中启用了提前停止, 可以用bst.best_iteration从最佳迭代中获得预测结果

```python
testing = np.random.rand(7, 10)
y_pred = bst.predict(testing, num_iteration = bst.best_iteration)
```

# 6.Random Forest

> * 随机森林是一种有监督学习算法，是以决策树为及学习器的集成学习算法。随机森林非常简单，易于实现，计算开销也很小，但是它在分类和回归上表现出惊人的性能，因此，随机森林被誉为“代表集成学习技术水平的方法”。
> * 随机森林(Random Forest,RF)是一种集成(ensemble)学习器，他利用Bootstrap重抽样方法从原始样本中抽取多个样本进行决策树(decision tree)建模，然后将这些决策树组合在一起，通过对所有决策树结果的平均(Mean)或投票(Vote)得出最终预测的回归(Regression)或分类(Classification)的结果。
> * 大量的理论和实证研究都证明了随机森林：
    - 随机森林既可以用于分类问题，也可以用于回归问题
    - 不容易出现过拟合
        - 过拟合是个关键的问题，可能会让模型在测试数据上的的结果变得糟糕，但是对于随机森林来说，如果随机森林的树足够多，那么分类器就不会过拟合模型
    - 具有较高的预测准确率
    - 随机森林可以用类别型特征建模
    - 随机森林可以处理缺失值
    - 对异常值和噪声数据具有很好的容忍度
    


## 6.1 随机森林的基学习器(base learner)

> 随机森林的基学习器就是没有剪枝的决策树


## 6.2 随机森林的随机性

> 随机森林的随机性体现在**数据集样本的随机抽样选择**和**待选特征的随机抽样选择**；

**数据集样本的随机抽样选择**


从原始的数据集中采取有放回的抽样(bagging)，构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。

**待选特征的随机抽样选择**

与数据集的随机选取类似，随机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选取最优的特征。


## 6.3 随机森林的构建过程

**参数符号使用声明：**

1. $m$: Bootstraping抽样的样本数量；
2. $n\_tree$: 构建的决策树数量；

**随机森林构建大致过程：**

1. 从原始训练数据集中使用Bootstraping方法随机有放回采样取出$M$个样本，共进行$n\_tree$次采样，生成$n\_tree$个训练集；
2. 对$n\_tree$个训练集，分别训练$n\_tree$个决策树模型；
3. 对于单个决策树模型，假设训练样本特征的个数为$n$，那么每次分裂时根据`信息增益/信息增益率/基尼指数`选择最好的特征进行分裂；
4. 每棵树都一直这样分裂下去，直到该节点的所有训练样本都属于同一类。在决策树的分裂过程中不需要剪枝；
5. 将生成的多棵决策树组成随机森林。对于分类问题，按照多棵树分类器投票决定最终分类结果；对于回归问题，由多克树预测值的均值决定最终预测结果；
6. 注意：OOB(out-of-bag)：每棵决策树的生成都需要自助采样，这时就有$\frac{1}{3}$的数据未被选中，这部分数据就称为**袋外数据**；


### 6.3.1 为每棵决策树Bootstrap抽样产生训练集

**(1) 每棵决策树都对应一个训练集数据，要构建$n\_tree$棵决策树，就需要产生对应数量($n\_tree$)的训练集，从原始训练集中产生$n\_tree$个训练子集要用到统计抽样技术。现有的统计抽样技术很多，按照抽样是否放回主要包括以下两种：**

* (i)不放回抽样(简单随机抽样)
    - 抽签法(小样本)
    - 随机数法(大样本)
* (ii)放回抽样
    - 无权重放回抽样(Bootstrap抽样)
        - 无权重抽样，也叫bagging方法。是一种用来提高学习算法准确度的方法。该方法于1996年由Breiman根据Boosting技术提出的。bagging方法是以可重复的随机抽样为基础的，每个样本是初始数据集有放回抽样。在可重复抽样生成多个训练子集时，存在于初始训练集D中的所有的样本都有可能被抽取的可能，但在重复多次后，总有一些样本是不能被抽取的，每个样本不能被抽取的概率为$(1-\frac{1}{N})^N$。
    - 有权重放回抽样
        - 有权重抽样，也叫boosting方法，也叫更新权重抽样。Boosting方法抽样，首先有放回随机抽样产生一组($n \leqslant N$)训练集，然后对这组训练集中每一个训练集设定权重为$\frac{1}{n}$，在设定权重后，对每个带权重的训练集进行测试(决策树训练)，在每次测试结束后，对分类性能差的训练集的权重进行提升，从而产生一个新的权重系列，经过多次训练后，每个训练集就有一个和其对应的权重，在投票时，这些权重就可以对投票的结果产生影响。从而影响最终的决策结果。

**(2) Bagging和Boosting方法都是可放回的抽样方法，但两者间存在很大的差别：**

* (i)Bagging方法在训练的过程中采用独立随机的方式。而Boosting方法在训练的过程中，每次训练都是在前一次的基础上进行的，因此是一种`串行`的关系，这对算法的执行是一个很大的挑战，以为每次执行都要等待上次的结果才能继续。而Bagging方法就不存在这个问题，这为算法的并行处理提供了很好的支持。
* (ii)Bagging方法抽取出来的训练集是没有权重的各训练集的待遇是相同的，而Boosting方法在抽取的过程中，对每个训练集都设置权重，使得抽取结束后每个训练集的待遇是不一致的。

**(3) 随机森林算法在生成的过程中，主要采用bagging方法，也就是Bootstrap抽样。**

* 从原始训练集中产生M个训练子集，每个训练子集的大小约为原始训练集的$\frac{2}{3}$，每次抽样均为随机且放回抽样，这样使得训练子集中的样本存在一定的重复，这样做的目的是为了使得随机森林中的决策树不至于产生局部最优解。

### 6.3.2 构建每棵决策树

> 随机森林算法为每个Bootstrap抽样训练子集分别建立一棵决策树，生成M棵决策树从而形成“森林”。每棵树任其生长，不需要剪枝。其中涉及两个主要过程：

**(1)节点分裂**

* **[决策树](http://rpubs.com/Wangzf/CART)**已经介绍了这一部分内容，随机森林常用的主要有C4.5, CART.

**(2)随机特征变量的随机选取**

* 随机特征变量是指随机森林算法在生成的过程中，参与**节点分裂属性(变量)比较**的**属性(变量)个数**。
* 由于随机森林在节点分裂时，不是所有的属性(变量)都参与属性(变量)指标的计算，而是随机地选择某几个属性(变量)参与比较，参与的属性个数就称之为随机特征变量。随机特征变量是为了使每棵决策树之间的相关性减少，同时提升每棵决策树的分类精度，从而提升整个随机森林的性能而引入的。其基本思想是，在进行节点分裂时，让所有的属性(变量)按照某种概率分布随机选择其中某几个属性参与节点分裂过程。在随机森林算法中，随机特征变量的产生方法主要有两种：
    - **随机选择输入变量(Forest-RI)**
        - Forest-RI是对输入变量(p个)随机分组(每组变量的个数F是一个定值)，然后对于每组变量，利用CART方法产生一棵树，并让其充分生长，不进行剪枝。在每个节点上，对输入该节点的变量，重复前面的随机分组，再重复CART方法，直到将所有节点均为叶节点为止。一般F有两种选择，首先是F=1，其次取F为小于$log_{2}(p+1)$的最大整数。假如只有很少的输入变量，比如p值不大，用Forest-RI法从p中随机选择F个作为随机特征变量，这样可能提高每棵树模型的精度，但同时也增大了各棵树之间的相关系数。
    - **随机组合输入变量(Forest-RC)**
        - Forest-RC是先将随机特征进行线性组合，然后再作为输入变量来构造随机森林的方法。
* 最常用的随机森林算法都是使用Forest-RI方法构建，在每棵子树的生长过程中，不是将全部p个输入变量参与节点分裂，而是随机抽取指定F($F \leqslant p$)个随机特征变量，F的取值一般为$log_{2}(p+1)$，以这F个属性上最好的分裂方式对节点进行分裂，从而达到节点分裂的随机性。

### 6.3.3 随机森林的形成

* 通过建立大量(M棵)的决策树，就形成了随机森林。算法最终的输出结果采取大多数投票法实现。根据随机构建的M决策子树将对某测试样本进行分类，将每棵子树的结果汇总，所得票数最多的分类结果将作为算法最终的输出结果。


## 6.4 随机森林的优缺点

**优点**

1. 由于采用了集成算法，本身精度比大多数单个算法要好，所以准确性高；
2. 在测试集上的表现良好，由于两个随机性的引入，使得随机森林不容易陷入过拟合(样本随机，特征随机)；
3. 在工业上，由于两个随机性的引入，使得随机森林具有一定的抗噪声能力，对比其他算法具有一定的优势；
4. 由于使用决策树的组合，使得随机森林可以处理非线性数据，本省属于非线性分类(拟合)模型；
5. 能够处理高维度的数据，并且不用做特征选择，对数据集的适应能力强：既能处理离散型数据，也能处理连续型数据，数据无需规范化；
6. 训练速度快可以运用在大规模数据集上；
7. 可以处理含有缺失值的特征(单独作为一类)，不用额外处理；
8. 由于有袋外数据(OOB)，可以在模型生成过程中取得真实误差的无偏估计，且不损失训练数据量；
9. 由于每棵树可以独立、同时生成，容易做成并行化方法；
10. 由于实现简单、精度高、抗过拟合能力强，当面对非线性数据时，适于作为基准模型；

**缺点**

1. 当随机森林中的决策树个数很多时，训练时需要的空间和时间会比较大；
2. 随机森林中还有很多不好解释的地方，有点算是黑盒模型；
3. 在某些噪音较大的样本集上，随机森林容易陷入过拟合；
4. **不能很好地处理非平衡数据**
    - 由于随机森林在构建过程中，训练集是随机选取的，使用Bootstrap随机抽样时，由于原训练集中的少数类本身就比较少，因此被选中的概率就很低，这使得M个随机选取的训练集中含有的少数类数量比原有的数据集更好或没有，这反而加剧了数据集的不平衡性，使得基于此数据集训练出来的决策树的规则就没有代表性
    - 由于数据集本身少数类占有的比例就低，使得训练出来的决策树不能很好地体现占有数量少的少数类的特点，只有将少数类的数量加大，使数据集中的数据达到一定的程度平衡，才能使得算法稳定
5. **需要对连续性变量进行离散化**
6. **随机森林的分类精度需要进一步提升**
    - 数据集的维度和样本的平衡性
    - 算法本身的决策树分裂规则、随机抽样



## 6.5 随机森林的特征选择功能

> * 用随机森林进行特征重要性评估的思想就是看每个特征在随机森林中的每棵树上做了多大的贡献，然后取平均值，最后比一比特征之间的贡献的大小； 
> * 特征在决策树上的贡献的大小度量通常使用`基尼指数(Gini Index)`或者`袋外数据(OOB)错误率`作为评估指标来衡量；

### 6.5.1 特征选择

#### 6.5.1.1 特征选择的步骤

在特征重要性的基础上，特征选择的步骤如下：

1. 计算每个特征的重要性，并按降序排列；
2. 确定要剔除的比例，依据特征重要性剔除相应比例的特征，得到一个新的特征集；
3. 用新的特征集重复上述过程，直到剩下$m$个特征($m$为提前设定的值)；
4. 根据上述过程得到的各个特征集合对应的OOB误差率，选择OOB误差率最低的特征集；


#### 6.5.1.2 特征重要性的估计方法

特征重要性的估计通常有两种方法：

1. 使用`uniform`或者`gaussian`抽取随机值替换特征；
2. 通过`permutation`的方式将原来的所有$N$个样本的第$i$个特征重新打乱分布；

第二种方法更加科学，保证了特征替代值与原特征的分布是近似的。这种方法叫做permutation test ，即在计算第i个特征的重要性的时候，将N 个特征的第i个特征重新洗牌，然后比较D和表现的差异性，如果差异很大，则表明第i个特征是重要的。



### 6.5.2 利用随机森林进行特征选择

```python
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier

url = "http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"

```

## 6.6 随机森林的性质、性能、参数

### 6.6.1 Random Forest 性质讨论

1. 随机森林泛化误差的收敛性
    - 随机森林中的决策树的泛化误差都收敛于：$\underset{n \rightarrow \infty}{\lim}PE^{*}=P_{xy}(P_{\Theta}(k(X,\Theta)=Y)-\underset{j\neq Y}{\max}P_{\Theta}(k(X,\Theta)\neq Y) > 0)$
    - 随着随机森林中决策树数量($n$)的增加，随机森林泛化误差($PE^{*}$)将趋向一个上界。随机森林对未知实例有很好的扩展性，也就是说随机森林随着决策树数量的增多不易过拟合
2. 随机森林中决策树的相关度和强度影响算法的泛化误差
    - 随机森林泛化误差的上界为：$PE^{*}\leqslant \frac{\bar{\rho}(1-s^{2})}{s^{2}}$
        - $\bar{\rho}$为决策树之间的相关度的平均值
        - $s$为决策树的平均强度
    - 要使随机森林的泛化性能好，则应该尽量减小决策树之间的相关性($\rho$)，增大单棵树的分类性能($s$)。每棵树的分类强度越大，则随机森林的分类性能越好，森林中树之间的相关度越小，则随机森林的分类性能越好

### **3.2 Random Forest 性能评价指标**

* **随机森林分类性能主要受内外两方面因素的影响：**
    - 外部因素：训练样本的正负类样本分布，即训练样本的平衡
    - 内部因素：单棵树的分类强度和树之间的相关度
* **衡量随机森林性能的指标：**
    - [分类效果]()
        - 分类精度:准确度(acccuracy of measurement), 是指使用算法得出的分类结果与真实之间的接近程度
        - 二分类数据的混淆矩阵
            - 分类精度(accuracy): $Accuracy=\frac{TP+TN}{TP+TN+FP+FN}$
            - 灵敏度(Sensitivity)(正类的的分类精度)：$Sensitivity=\frac{TP}{TP+FN}$
            - 特异度(Specificity)(负类的的分类精度)：$Specificity=\frac{TN}{FP+TN}$
            - 几何均值(G-mean)：$G-mean=\sqrt{\frac{TP}{TP+FN}\times \frac{TN}{FP+TN}}$
            - 负类的查全率(Recall)：$Recall=\frac{TP}{TP+FN}$
            - 负类的查准率(Precision)：$Precision=\frac{TP}{TP+FP}$
            - 负类的检验值(F-value)：$F-value=\frac{(1+\beta^{2})\cdot recall \cdot precision}{\beta^{2}\cdot recall \cdot precision}, \beta \in (0, 1]$
    - 泛化误差
        - 泛化能力(generalization ability)
        - 泛化误差(generalization error)
            - 泛化误差是反应泛化能力的一个指标
            - 随机森林的泛化误差理论上是可以计算出来的，然而，在实际环境中，样本的期望输出和分布情况都是不知道的，无法直接通过计算泛化误差来评估随机森林的泛化能力
            - 估计泛化误差：
                - 交叉验证(Cross-Validation, CV)(验证集上的)
                    - 运算量很大
                - OOB估计
                    - 随机森林是使用Bootstrap来进行每棵树训练集的生成，在生成这些(M)个训练集时，初始训练集中有一些样本是不能被抽取的这些样本的个数是初始数据集的$(1-\frac{1}{N})^N$。可以证明，当$N$足够大时，$(1-\frac{1}{N})^N$将收敛于$\frac{1}{e}\approx 0.368$，说明将有近$37\%$的样本不会被抽取出来，这些不能被抽取的样本组成的集合，称之为袋外数据(OOB)。
                    - 使用OOB数据来估计随机森林算法的泛化能力称为OOB估计：以每棵决策树为单位，利用OOB数据统计该树的OOB误分率；将所有决策树的误分率取平均得到随机森林的OOB误分率，就可以得到一个OOB误差估计。
                    - Breiman通过实验已经证明，OOB估计是随机森林的泛化误差的一个无偏估计
                    - 相比于CV估计，效率很高
        
    - 运行效率

### 6.6.2 随机森林参数设置

* **随机森林算法中需要设置的主要参数：**
    - 随机森林中决策树的数量(ntree)
    - 随机森林内部节点随机选择属性的个数(mtry)： 一般为小于$log_{2}(p+1)$的最大整数
* 一般来讲，决策树的数量越多，算法的精度越高，但程序的速度会有所下降；
* 内部节点随机选择属性的个数(mtry)是影响算法精度的主要因子，随机森林内决策树的强度和相关度和随机选择属性的个数相关，如果随机选择属性的个数足够小，树的相关性趋向于减弱，另一方面，决策树模型的分类强度随着随机选择属性的个数的增加而提高。

